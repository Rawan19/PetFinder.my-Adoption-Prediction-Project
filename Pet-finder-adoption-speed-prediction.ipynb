{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n      #  print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport re\nimport html\nimport unicodedata\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import  LogisticRegressionCV\nimport gc\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 100)\nimport os\nfrom sklearn.metrics import cohen_kappa_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breeds = pd.read_csv('../input/petfinder-adoption-prediction/BreedLabels.csv')\ncolors = pd.read_csv('../input/petfinder-adoption-prediction/ColorLabels.csv')\nstates = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')\n\ntrain = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv', engine='python')\nsub = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_features=pd.read_csv(\"../input/train-img-features/train_img_features.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_features.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"PetID\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img_features=pd.read_csv(\"../input/testimgfeatures/test_img_features.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"PetID\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img_features.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_img_features.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_features.set_index(\"Unnamed: 0\",inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img_features.set_index(\"Unnamed: 0\",inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_features.columns = [\"img_feat{}\".format(i) for i in range(256)]\ntest_img_features.columns = [\"img_feat{}\".format(i) for i in range(256)]\n\ntrain_img_features[\"PetID\"] = train_img_features.index\ntest_img_features[\"PetID\"] = test_img_features.index\n\ntrain = pd.merge(train, train_img_features, on=\"PetID\")\ntest = pd.merge(test, test_img_features, on=\"PetID\")\n\nprint(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find+clean nulls","metadata":{}},{"cell_type":"code","source":"\n    ##\n\ntrain.loc[:,\"Description\"] = train.loc[:,\"Description\"].fillna(\"MISSINGG\")\n    \n#train_proc.dropna(axis=0,inplace=True)\ntest.loc[:,\"Description\"] = test.loc[:,\"Description\"].fillna(\"MISSINGG\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #feature engineering","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntrain['Name'] = train['Name'].fillna('No_name_found')\ntest['Name'] = test['Name'].fillna('No_name_found')\n#all_data['Name'] = all_data['Name'].fillna('No_name_found')\n\ntrain['No_name'] = 0\ntrain.loc[train['Name'] == 'No_name_found', 'No_name'] = 1\ntest['No_name'] = 0\ntest.loc[test['Name'] == 'No_name_found', 'No_name'] = 1\n#all_data['No_name'] = 0\n#all_data.loc[all_data['Name'] == 'No_name_found', 'No_name'] = 1\n\n\ntrain['Pure_breed'] = 0\ntrain.loc[train['Breed2'] == 0, 'Pure_breed'] = 1\n\ntest['Pure_breed'] = 0\ntest.loc[test['Breed2'] == 0, 'Pure_breed'] = 1\n\ntrain[\"desc_length\"]=train[\"Description\"].apply(lambda x: len(x.split()))\ntest[\"desc_length\"]=test[\"Description\"].apply(lambda x: len(x.split()))\n#all_data['Pure_breed'] = 0\n#all_data.loc[all_data['Breed2'] == 0, 'Pure_breed'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"desc_length\"][8]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Description\"][8]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['dataset_type'] = 'train'\ntest['dataset_type'] = 'test'\nall_data = pd.concat([train, test])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parsing sentiment data","metadata":{}},{"cell_type":"markdown","source":"information about Google's  NLP API:\nhttps://cloud.google.com/natural-language/docs/basics#sentiment-analysis-values\n\nThe score of a document's sentiment indicates the overall emotion of a document. The magnitude of a document's sentiment indicates how much emotional content is present within the document, and this value is often proportional to the length of the document.v\n\nA document with a neutral score (around 0.0) may indicate a low-emotion document, or may indicate mixed emotions, with both high positive and negative values which cancel each out. Generally, you can use magnitude values to disambiguate these cases, as truly neutral documents will have a low magnitude value, while mixed documents will have higher magnitude values.","metadata":{}},{"cell_type":"code","source":"#This code was  inspired from this amazing repo:\n#https://github.com/liyi61/PetFinder.my-Adoption-Speed-Prediction/tree/master\nsentiment_dict={}\nfor filename in os.listdir(\"../input/petfinder-adoption-prediction/train_sentiment/\"):\n    with open(\"../input/petfinder-adoption-prediction/train_sentiment/\"+filename, 'rb') as f:\n        sentiment = json.load(f)\n    pet_id=filename.split('.')[0]\n    sentiment_dict[pet_id]={}\n    sentiment_dict[pet_id][\"magnitude\"]=sentiment[\"documentSentiment\"][\"magnitude\"]\n    sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\n    sentiment_dict[pet_id]['language'] = sentiment['language']\n#\n\nfor filename in os.listdir('../input/petfinder-adoption-prediction/test_sentiment/'):\n    with open('../input/petfinder-adoption-prediction/test_sentiment/' + filename, 'rb') as f:\n        sentiment = json.load(f)\n    pet_id = filename.split('.')[0]\n    sentiment_dict[pet_id] = {}\n    sentiment_dict[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\n    sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\n    sentiment_dict[pet_id]['language'] = sentiment['language']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['lang'] = train['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\ntrain['magnitude'] = train['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\ntrain['score'] = train['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\n\ntest['lang'] = test['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\ntest['magnitude'] = test['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\ntest['score'] = test['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\n\nall_data['lang'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\nall_data['magnitude'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\nall_data['score'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parsing img metadata","metadata":{}},{"cell_type":"markdown","source":"Let's take an idea about what the vision API does;\n\n1.**labelAnnotations**: Labels can identify general objects, locations, activities, animal species, products, and more. \nA LABEL_DETECTION response includes the detected labels, their score, topicality, and an opaque label ID, where:\n(https://cloud.google.com/vision/docs/labels)\n       mid - if present, contains a machine-generated identifier (MID) corresponding to the entity's Google Knowledge Graph entry. Note that mid values remain unique across different languages, so you can use these values to tie entities together from different languages. To inspect MID values, refer to the Google Knowledge Graph API documentation.\n   \n    description - the label description.\n    \n    score - the confidence score, which ranges from 0 (no confidence) to 1 (very high confidence).\n    (scores are stored from the highest to the lowest. therefore, I'll only consider the **1st description** for each image. This explains why we're indexing [\"labelAnnotations\"][score] **[0]**\n    topicality - The relevancy of the ICA (Image Content Annotation) label to the image. It measures how important/central a label is to the overall context of a page.\n\n\n2.**imagePropertiesAnnotation**(https://cloud.google.com/vision/docs/detecting-properties)\n\nThe Image Properties feature detects general attributes of the image, such as dominant color.\n\na.dominant color>>colors>>scores(https://github.com/googleapis/google-cloud-node/issues/1324)\n:, higher \"scores\" means higher confidence that the color in question is prominent in the central focus of the image,\nagain,sorted from the highest to  lowest.\n\nb.dominant color>>colors>>pixelFraction \tStores the fraction of pixels the color occupies in the image. Value in range [0, 1]. \n\n3.**cropHintsAnnotation**\ncropHints>>boundingPoly: vertices for a bounding polygon for the detected image annotation. \ncropHints>>ImportanceFraction: Fraction of importance of this salient(polygon) region with respect to the original image.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"lang\"].value_counts()#we can drop this column since english dominates other languages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_use = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n       'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'RescuerID', 'score',\n       'VideoAmt', \"Description\", 'PhotoAmt', 'AdoptionSpeed', 'magnitude']\n#feature engineering; 'health', 'Free',, 'No_name', 'Pure_breed', 'desc_length','desc_words',\n          #     'average_word_length'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cols_to_use)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train[[col for col in cols_to_use if col in train.columns]]\n# test = test[[col for col in cols_to_use if col in test.columns]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n       'Sterilized', 'Health', 'State', 'RescuerID',\"lang\"]\n#Fee,Age,'VideoAmt', 'PhotoAmt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# more_cols = []\n# #feature interaction\n# for col1 in cat_cols:\n#     for col2 in cat_cols:\n#         if col1 != col2 and col1 not in ['RescuerID', 'State'] and col2 not in ['RescuerID', 'State']:\n#             train[col1 + '_' + col2] = train[col1].astype(str) + '_' + train[col2].astype(str)\n#             test[col1 + '_' + col2] = test[col1].astype(str) + '_' + test[col2].astype(str)\n#             more_cols.append(col1 + '_' + col2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cat_cols = cat_cols + more_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexer = {}\nfor col in cat_cols:\n    # print(col)\n    _, indexer[col] = pd.factorize(train[col].astype(str))\n  #  train.loc[:,col]=pd.factorize(train.loc[:,col])[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_cols:\n    # print(col)\n    train[col] = indexer[col].get_indexer(train[col].astype(str))\n    test[col] = indexer[col].get_indexer(test[col].astype(str))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train['AdoptionSpeed']\ntrain = train.drop(['AdoptionSpeed'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split, cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(train, y_train, test_size = 0.2,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data =x_train.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label0 = y_train.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label0.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_label=np.zeros([train_label0.shape[0],5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convertLabels 2 1-hotEncoded\nfor i in range(train_label0.shape[0]):\n    if train_label0[i] == 0:\n        train_label[i, 0] = 1\n    elif train_label0[i] == 1:\n        train_label[i, 1] = 1\n    elif train_label0[i] == 2:\n        train_label[i, 2] = 1\n    elif train_label0[i] == 3:\n        train_label[i, 3] = 1\n    elif train_label0[i] == 4:\n        train_label[i, 4] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_data = x_valid.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_label0 = y_valid.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model parameters","metadata":{}},{"cell_type":"code","source":"num_examples_per_epoch_for_train = train_data.shape[0]\ntraining_epochs = 70\nbatch_size = 1200\ndisplay_step = 1\nLR = 0.0001","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalization\ndef norm(inputs, on_train):\n    conv_mean, conv_var = tf.nn.moments(inputs, [0, 1, 2])\n\n    scale = tf.Variable(tf.ones([inputs.shape[-1]]))\n    shift = tf.Variable(tf.zeros([inputs.shape[-1]]))\n    epsilon = 0.001\n    ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n    def mean_var_with_update():\n        ema_apply_op = ema.apply([conv_mean, conv_var])\n        with tf.control_dependencies([ema_apply_op]):\n            return tf.identity(conv_mean), tf.identity(conv_var)\n\n    mean, var = tf.cond(on_train, mean_var_with_update, lambda: (ema.average(conv_mean), ema.average(conv_var)))\n    norm_out = tf.nn.batch_normalization(inputs, mean, var, shift, scale, epsilon)\n    return norm_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Drop out nodes to avoid overfitting \ndef Dropout(x, rate):\n    with tf.name_scope('Dropout_layer'):\n        return tf.nn.dropout(x, rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Convolutional Layers\ndef add_layer(input, W_shape, b_shape, strides,  on_train,  layer_name, padding='SAME', activation_function=tf.nn.tanh):\n    with tf.name_scope(layer_name):\n        with tf.name_scope('Weights'):\n        # Initialize weights\n            Weight = tf.Variable(tf.truncated_normal(shape=W_shape, stddev=0.1, dtype=tf.float32))\n        with tf.name_scope('Biases'):\n        \n            b = tf.Variable(tf.constant(0.01, shape=b_shape))\n        with tf.name_scope('Conv'):\n        \n            conv_out = tf.nn.conv2d(input, Weight, strides, padding)\n            conv_out_plus_b = tf.nn.bias_add(conv_out, b)\n     \n        with tf.name_scope('Activation'):\n        #  activation, add non linear relationship\n            x = activation_function(conv_out_plus_b)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ddfine input placeholder \nwith tf.name_scope('Inputs'):\n    x_placeholder = tf.placeholder(tf.float32, [None, 294])\n    x = tf.reshape(x_placeholder, [-1, 294, 1, 1])\n    y = tf.placeholder(tf.int32, [None, 5])\n    on_train = tf.placeholder(tf.bool)\n    rate_place = tf.placeholder(tf.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# convolutioanl layer 1 \nconv1 = add_layer(x, [3, 1, 1, 16], [16], [1, 1, 1, 1], on_train, 'Conv1_1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ResNet Convolutional Block 1 (including two 3*1 convolutional layers)\nwith tf.name_scope('Res_block1'):\n    conv1_2 = add_layer(conv1, [3, 1, 16, 16], [16], [1, 1, 1, 1], on_train, 'Conv1_2')\n    conv1_3 = add_layer(conv1_2, [3, 1, 16, 16], [16], [1, 1, 1, 1], on_train, 'Conv1_3',\n                        activation_function=tf.identity)\n    fuse1 = tf.nn.tanh(tf.add(conv1, conv1_3))\n\ntrans1 = add_layer(fuse1, [3, 1, 16, 64], [64], [1, 1, 1, 1], on_train, 'Tans1')\n\n# ResNet Convolutional Block 2 (including two 3*1 convolutional layers)\nwith tf.name_scope('Res_block2'):\n    conv2_1 = add_layer(trans1, [3, 1, 64, 64], [64], [1, 1, 1, 1], on_train, 'Conv2_1')\n    conv2_2 = add_layer(conv2_1, [3, 1, 64, 64], [64], [1, 1, 1, 1], on_train, 'Conv2_2',\n                        activation_function=tf.identity)\n    fuse2 = tf.nn.tanh(tf.add(trans1, conv2_2))\n\ntrans2 = add_layer(fuse2, [3, 1, 64, 128], [128], [1, 1, 1, 1], on_train, 'Tans2')\n\n# ResNet Convolutional Block 3 (including two 3*1 convolutional layers)\nwith tf.name_scope('Res_block3'):\n    conv3_1 = add_layer(trans2, [3, 1, 128, 128], [128], [1, 1, 1, 1], on_train, 'Conv3_1')\n    conv3_2 = add_layer(conv3_1, [3, 1, 128, 128], [128], [1, 1, 1, 1], on_train, 'Conv3_2',\n                        activation_function=tf.identity)\n    fuse3 = tf.nn.tanh(tf.add(trans2, conv3_2))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorize features 37632\nfuse_F_flat = tf.reshape(fuse3, [-1, 37632])\n\n# Fully Connected 1\nwith tf.name_scope('FC1_layer'):\n    with tf.name_scope('W_fc1'):\n        W_fc1 = tf.Variable(tf.truncated_normal(shape=[37632, 1024], stddev=0.1, dtype=tf.float32))\n    with tf.name_scope('b_fc1'):\n        b_fc1 = tf.Variable(tf.constant(0.01, shape=[1024]))\n    fc1 = tf.nn.tanh(tf.matmul(fuse_F_flat, W_fc1) + b_fc1)\n    fc1_drop = Dropout(fc1, rate_place)\n# Fully Connected 2\nwith tf.name_scope('FC1_layer'):\n    with tf.name_scope('W_fc2'):\n        W_fc2 = tf.Variable(tf.truncated_normal(shape=[1024, 1024], stddev=0.1, dtype=tf.float32))\n    with tf.name_scope('b_fc2'):\n        b_fc2 = tf.Variable(tf.constant(0.01, shape=[1024]))\n    fc2 = tf.nn.tanh(tf.matmul(fc1_drop, W_fc2) + b_fc2)\n    fc2_drop = Dropout(fc2, rate_place)\n# Fully Connected 3\nwith tf.name_scope('FC1_layer'):\n    with tf.name_scope('W_fc3'):\n        W_fc3 = tf.Variable(tf.truncated_normal(shape=[1024, 512], stddev=0.1, dtype=tf.float32))\n    with tf.name_scope('b_fc3'):\n        b_fc3 = tf.Variable(tf.constant(0.01, shape=[512]))\n    fc3 = tf.nn.tanh(tf.matmul(fc1_drop, W_fc3) + b_fc3)\n    fc3_drop = Dropout(fc3, rate_place)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Prediction Output Layer with output dimendion 5\nwith tf.name_scope('Output'):\n    with tf.name_scope('W_fc4'):\n        W_fc4 = tf.Variable(tf.truncated_normal(shape=[512, 5], stddev=0.1, dtype=tf.float32))\n    with tf.name_scope('b_fc4'):\n        b_fc4 = tf.Variable(tf.constant(0.01, shape=[5]))\n    y_pred = tf.matmul(fc3_drop, W_fc4) + b_fc4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use Cross-entropy as loss function \nwith tf.name_scope('Loss'):\n    loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=y_pred)\n\nwith tf.name_scope('Train'):\n    train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To calculate accuracy\nwith tf.name_scope('Accuracy'):\n    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GT = tf.argmax(y, 1)\nPD = tf.argmax(y_pred, 1)\n\n\ninit_op = tf.global_variables_initializer()\n\n# to save model\nsaver = tf.train.Saver(tf.global_variables())\n\n# Start Session，run the network\nwith tf.Session() as sess:\n    # write model and parameters into ckpt file \n    ckpt = tf.train.get_checkpoint_state('./model')\n    \n    sess.run(init_op)\n    # number of batches\n    total_batches = int(num_examples_per_epoch_for_train / batch_size)\n    print('Per batch Size:', batch_size)\n    print('Train sample Count Per Epoch:', num_examples_per_epoch_for_train)\n    print('Total batch Count Per Epoch:', total_batches)\n    training_step = 0\n    # training \n    for epoch in range(training_epochs):\n        for batch_idx in range(total_batches):\n            start_idx = batch_idx * batch_size\n            end_idx = start_idx + batch_size\n            # training batch\n            batch_xs = train_data[start_idx:end_idx, :]\n            batch_ys = train_label[start_idx:end_idx, :]\n\n            row_rand_array = np.arange(test_data.shape[0])\n            np.random.shuffle(row_rand_array)\n            batch_xs_val = test_data[row_rand_array[0:batch_size], :]\n            batch_ys_val = test_label[row_rand_array[0:batch_size], :]\n\n            # loss\n            _, loss_ = sess.run([train_op, loss], {x_placeholder: batch_xs, y: batch_ys, on_train: True, rate_place: 0.7})\n            training_step += 1\n            \n            if training_step % display_step == 0:\n                \n                result, gt = sess.run([PD, GT], feed_dict={x_placeholder: batch_xs, y: batch_ys, on_train: False, rate_place: 1})\n                train_accuracy = kappa(gt, result)\n\n                result_val, gt_val = sess.run([PD, GT], feed_dict={x_placeholder: batch_xs_val, y: batch_ys_val, rate_place: 1,\n                    on_train: False})\n                \n                valid_accuracy = kappa(gt_val, result_val)\n\n                # test\n                print('Epoch:', epoch, '| Step:', training_step, '| train loss: %.4f' % loss_,\n                      '| train accuracy: %.4f' % train_accuracy,  '| valid accuracy: %.4f' % valid_accuracy,)\n        # save\n        saver.save(sess=sess, save_path='./model/prediction.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TextCol(Descriptio)","metadata":{}},{"cell_type":"markdown","source":"## Text cleaning(Descriptio)","metadata":{}},{"cell_type":"raw","source":"Replace   nulls","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\ndef remove_html(text):\n    html=re.compile(r\"<.*?>\")\n    return  html.sub(r'',  text)\n   # This should happen before all other preprocessing steps,\n   # as we will see in the full pipeline, since it will help sentence and words tokenization for example, and will reduce vocab. \n   \n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\n#7599:#Earthquake\n#other   examples: earthquake, EARTHQUAKE\n\ndef to_lowercase(text):\n    return text.lower()\n\n#remove URLs\ndef remove_urls(text):\n  #slower than regex\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef clean_text( text):\n    text = remove_special_chars(text)\n    text=remove_html(text)\n    text=remove_urls(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    \n\n  #  text = replace_numbers(text)\n    words = text2words(text)\n    #REMOVE STOPWORDS?\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descrip_clean=train[\"Description\"].apply(lambda x:clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descrip_clean_test=test[\"Description\"].apply(lambda x:clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Description_clean\"]=descrip_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"Description_clean\"]=descrip_clean_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"Description_clean\"]==\"missingg\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"lang\"]!=\"en\"][\"Description\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# encoding description","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bow_tf = TfidfVectorizer( ngram_range=(1,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bow_tf_test = TfidfVectorizer( ngram_range=(1,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_text_tfidf = bow_tf.fit_transform(train[\"Description_clean\"])##extTrial","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_text_tfidf_test = bow_tf.fit_transform(test[\"Description_clean\"])##extTrial","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_text_tfidf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_features=[]\nn_components = 5\n\nsvd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\nsvd_col = svd_.fit_transform(x_text_tfidf)\nsvd_col = pd.DataFrame(svd_col)\n  # print(svd_col)\nsvd_col = svd_col.add_prefix('SVD_Description_')\n\n#    nmf_col = nmf_.fit_transform(tfidf_col)\n#    nmf_col = pd.DataFrame(nmf_col)\n#    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n\ntext_features.append(svd_col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\ntext_features.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate with main DF:\ntrai_svd=pd.concat([train, text_features], axis=1)\n#test_svd=pd.concat([test, text_features], axis=1)\n#trai_svd.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#same4test\ntext_features=[]\nn_components = 5\n\nsvd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    #Non-Negative Matrix Factorization (NMF).\n    #Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. \n#This factorization can be used for example for dimensionality reduction, source separation or topic extraction.\n#     nmf_ = NMF(\n#         n_components=n_components, random_state=1337)\nsvd_col = svd_.fit_transform(x_text_tfidf_test)\nsvd_col = pd.DataFrame(svd_col)\n  # print(svd_col)\nsvd_col = svd_col.add_prefix('SVD_Description_')\n\n#    nmf_col = nmf_.fit_transform(tfidf_col)\n#    nmf_col = pd.DataFrame(nmf_col)\n#    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n\ntext_features.append(svd_col)\n# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\ntext_features.shape\n# Concatenate with main DF:\ntest_svd=pd.concat([test, text_features], axis=1)\ntest_svd.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_svd=pd.concat([test, text_features], axis=1)\n# test_svd.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#descr_tf=pd.DataFrame(x_text_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_text=trai_svd[\"Description\"]\n\ntrai_svd.drop([\"Description\", \"Description_clean\"], axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_text=trai_svd[\"Description\"]\n\ntest_svd.drop([\"Description\", \"Description_clean\"], axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.drop(\"PetID\",axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_svd.drop(\"PetID\",axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.drop(\"Description\", inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test.drop(\"Description\", inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train/test split","metadata":{}},{"cell_type":"markdown","source":"since dataset size is small,the validation_set won't be representative to estimate val_score on","metadata":{}},{"cell_type":"markdown","source":"# modeling","metadata":{}},{"cell_type":"code","source":"n_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.drop([\"Name\", \"dataset_type\"],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_svd.drop([\"Name\", \"dataset_type\"],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trai_svd.drop([\"lang\"],axis=1,inplace=True)\n# test_svd.drop([\"lang\"],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_svd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.drop(\"Description_clean\", axis=1, inplace=True)\ntest_svd.drop(\"Description_clean\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(X=trai_svd, X_test=test_svd, y=y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False,\n                averaging='usual', make_oof=False):\n    result_dict = {}\n    if make_oof:\n        oof = np.zeros((len(X), 5))\n    prediction = np.zeros((len(X_test), 5))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        gc.collect()\n        print('Fold', fold_n + 1, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_cols)\n            valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_cols)\n\n            model = lgb.train(params,\n                              train_data,\n                              num_boost_round=20000,\n                              valid_sets=[train_data, valid_data],\n                              verbose_eval=500,\n                              early_stopping_rounds=200)\n\n            del train_data, valid_data\n\n            y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n            del X_valid\n            gc.collect()\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200,\n                              verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)\n            \n        if model_type == 'lcv':\n            model = LogisticRegressionCV(scoring='neg_log_loss', cv=3, multi_class='multinomial')\n            model.fit(X_train, y_train)\n\n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test)\n\n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=500, loss_function='MultiClass')\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[],\n                      use_best_model=False, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict_proba(X_test)\n\n        if make_oof:\n            oof[valid_index] = y_pred_valid\n            \n            \n        scores.append(kappa(y_valid, y_pred_valid.argmax(1)))\n        print('Fold kappa:', kappa(y_valid, y_pred_valid.argmax(1)))\n        print('')\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values\n\n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n\n    if model_type == 'lgb':\n\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_fold\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12))\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n            plt.title('LGB Features (avg over folds)')\n\n            result_dict['feature_importance'] = feature_importance\n\n    result_dict['prediction'] = prediction\n    if make_oof:\n        result_dict['oof'] = oof\n\n    return result_dict\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#########lightgbm###############\nprint('----------training lightgbm-------------')\nparams = {'num_leaves': 512,\n        #  'min_data_in_leaf': 60,\n         'objective': 'multiclass',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 3,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"random_state\": 42,\n         \"verbosity\": -1,\n         \"num_class\": 5}\nresult_dict_lgb = train_model(X=trai_svd, X_test=test_svd, y=y, params=params, model_type='lgb', plot_feature_importance=False, make_oof=True)\nprediction_lgb = (result_dict_lgb['prediction']).argmax(1)\nsubmission_lgb = pd.DataFrame({'PetID': sub.PetID, 'AdoptionSpeed': [int(i) for i in prediction_lgb]})\nsubmission_lgb.head()\nsubmission_lgb.to_csv('submission.csv', index=False)\nprint('----------train lightgbm end-------------')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## retraining on the entire train data","metadata":{}},{"cell_type":"markdown","source":"crossvalidation is for model selection only!\nOnce decided (among types, hyper-params, data pipeline)\n→ Use the whole training set to train the selected (type, set of hyperparams and data pipeline)\n","metadata":{}},{"cell_type":"code","source":"#  train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_cols)\n#             valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_cols)\n\n#             model = lgb.train(params,\n#                               train_data,\n#                               num_boost_round=20000,\n#                               valid_sets=[train_data, valid_data],\n#                               verbose_eval=500,\n#                               early_stopping_rounds=200)\n\n#             del train_data, valid_data\n\n#             y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n#             del X_valid\n#             gc.collect()\n#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ##############xgboost##################\n# print('----------training xgboost-------------')\n# xgb_params = {'eta': 0.01, 'max_depth': 9, 'subsample': 0.9, 'colsample_bytree': 0.9,\n#           'objective': 'multi:softprob', 'eval_metric': 'merror', 'silent': True, 'nthread': 4, 'num_class': 5}\n# result_dict_xgb = train_model(params=xgb_params, model_type='xgb', make_oof=True)\n# prediction_xgb= (result_dict_xgb['prediction']).argmax(1)\n# submission_xgb = pd.DataFrame({'PetID': sub.PetID, 'AdoptionSpeed': [int(i) for i in prediction_xgb]})\n# submission_xgb.head()\n# submission_xgb.to_csv('submission_xgb.csv', index=False)\n# print('----------train xgboost end-------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}